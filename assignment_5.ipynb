{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AAHPS: ASSIGNMENT 5**\n",
    "\n",
    "**Authors**: Nina Mislej and Nika Molan\n",
    "<br/>**Student numbers**: 63200016 and 63200017\n",
    "\n",
    "### **Exercise Description**\n",
    "The aim of this assignment is to find the best result for **24 optimization** (*minimization*) **functions** that are available as **BBOB** (*Black-Box Optimization Benchmark*). We will implement **4 optimization programs** one of which has to be **local search** and the other ones can implement **any** optimization approach.\n",
    "<br/>The functions are available in ***smoof*** package in **R** and we will initialize the functions with **40 dimensions** using the *iid*: **2023**\n",
    "\n",
    "In addition to this report the results include the **coordinates** for each of the **24 minimums**. These are included in a separate file, one for each algorithm. \n",
    "<br/>One line represents **40**-**touple** for one function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Functions Description**\n",
    "\n",
    "The **[functions](https://numbbo.github.io/gforge/downloads/download16.00/bbobdocfunctions.pdf)** are designed to cover a lot of different problems that occur with **different optimization approaches**. Some of the functions are meant to test if the algorithm gets **stuck in local optimums**, some check if the algorithm can find optimums **bordering** the function domain, some functions are symetric while some highly asymetric and so on and so forth. We have to take this properties in to account when chosing our algorithms to optimize the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the package and enviroment\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from rpy2.robjects import numpy2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "numpy2ri.activate() # automatic conversion from numpy to R arrays\n",
    "smoof = importr(\"smoof\") # importing R smoof package"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we **initialize** the functions.\n",
    "</br>In this process we also save their known minimums, which are provided in their description. We will use these minimums later on **to compare** the coordinates we get from our optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINIMUM OF FUNCTION 1: 21.1\n",
      "MINIMUM OF FUNCTION 2: 26.91\n",
      "MINIMUM OF FUNCTION 3: 311.6\n",
      "MINIMUM OF FUNCTION 4: 311.6\n",
      "MINIMUM OF FUNCTION 5: -48.47\n",
      "MINIMUM OF FUNCTION 6: -91.36\n",
      "MINIMUM OF FUNCTION 7: 32.49\n",
      "MINIMUM OF FUNCTION 8: 71.6\n",
      "MINIMUM OF FUNCTION 9: -356.7\n",
      "MINIMUM OF FUNCTION 10: 51.03\n",
      "MINIMUM OF FUNCTION 11: -96.65\n",
      "MINIMUM OF FUNCTION 12: 553.39\n",
      "MINIMUM OF FUNCTION 13: 9.88\n",
      "MINIMUM OF FUNCTION 14: 405.47\n",
      "MINIMUM OF FUNCTION 15: 64.25\n",
      "MINIMUM OF FUNCTION 16: -43.28\n",
      "MINIMUM OF FUNCTION 17: 227.51\n",
      "MINIMUM OF FUNCTION 18: 227.51\n",
      "MINIMUM OF FUNCTION 19: 73.06\n",
      "MINIMUM OF FUNCTION 20: -123.81\n",
      "MINIMUM OF FUNCTION 21: -44.42\n",
      "MINIMUM OF FUNCTION 22: 222.1\n",
      "MINIMUM OF FUNCTION 23: -1000.0\n",
      "MINIMUM OF FUNCTION 24: -1.33\n"
     ]
    }
   ],
   "source": [
    "# initializing the functions and checking their minimums\n",
    "functions = {}\n",
    "true_minimums = {}\n",
    "\n",
    "for fun in range(1, 25):\n",
    "    functions[fun] = (smoof.makeBBOBFunction(40, fun, 2023))\n",
    "    true_minimums[fun] = smoof.getGlobalOptimum(functions[fun]).rx2(\"value\")[0]\n",
    "    print(f\"MINIMUM OF FUNCTION {fun}: {true_minimums[fun]}\")\n",
    "\n",
    "# setting the bounds:\n",
    "upper_bound = 5\n",
    "lower_bound = -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing minimums we got with the real ones\n",
    "def comparison(approx_minimums):\n",
    "    error = 0\n",
    "    for i,(true,approx) in enumerate(zip(true_minimums.values(), approx_minimums),1):\n",
    "        print(f\"{i : <20} \\\n",
    "              TRUE: {true : <20} \\\n",
    "              APPROXIMATE: {round(approx,2) : <20} \\\n",
    "              DIFFERENCE: {round(abs(true - approx),2)}\")\n",
    "        \n",
    "        error = error + round(abs(true - approx),2)\n",
    "    print(f\"OVERALL ABSOLUTE ERROR:  {error}\")\n",
    "\n",
    "# writing the coordinates of the minimum to file\n",
    "def results_to_file(coordinates, algo_number):\n",
    "    with open(f\"algorithm_{algo_number}.txt\", 'w') as f:\n",
    "        for point in coordinates:\n",
    "            for xi in point:\n",
    "                f.write(f\"{xi} \")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ALGORITHM 1:** Gradient Descent\n",
    "\n",
    "This algorithm was chosen as the **first one** we tested, under the assumtion it is often wildly used and works for functions that behave well. One big problem is the differentiability of the functions which is not always favourable and is sometimes time consuming so we used an **approximation**. 15 out of the 17 functions with description are described as **differentiable** so it is worth analysing this approach, even though it is a bit naive because it only performs well for smooth, convex, unimodal functions. The problem with this aproach is also that it has a big possibility of getting stuck in a **local minimum**. We will be solving this problem by adding a random component to it, making the algorithm go through **more iterations** starting at **different points** chosen uniformly at random.\n",
    "\n",
    "The idea is that we start in a randomly chosen point and compute the **gradient** for that set of coordinates. The function **``gradient_approximation``** takes care of that. Instead of differentiating each variable, we use two points that are 0.01 apart. Because this is the direction of the **biggest increase** for the function we are trying to optimize we compute the next point by going in the opposite direction of the mentioned increase. This is the equation $x_n = x_{n-1} - \\alpha \\cdot \\bigtriangleup G(x_n)$ where $\\bigtriangleup G$ is the gradient of the objective function.\n",
    "\n",
    "The next question that arises in this proposition is what is the **step size** in this direction? Let's look at the **``alpha``** parameter in the code bellow. Now there are many ways to find this scalar, the optimal one would be finding the one that **minimizes** the equation: $G(x_{n-1} - \\alpha \\cdot \\bigtriangleup G(x_n))$. This would take a lot of time because we are working with 40 dimentions and after some observations one can notice the elements of the gradients sometimes tend to be very different in size. We tackled with this problem by making the step size for each partial derivative different making it **between 0 and 1** for every single one. This also solves a lot of domain breach problems. We wrapped all of this up in a function that creates the new point **``new_point``**.\n",
    "\n",
    "***NOTE***: This algorithm is quite **time consuming** for all 24 functions, because we have to calculate the **gradient approximation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function implementation\n",
    "def gradient_descent(limit, dim, fun):\n",
    "    \n",
    "    # setting the best solution to the first one we try and updating it later on \n",
    "    # this is not neccessary and could be omited \n",
    "    # but given our lambda calculation we could skip to a worse solution\n",
    "    best_coordinates = np.random.uniform(lower_bound, upper_bound, dim)\n",
    "    best_value = fun(best_coordinates)[0]\n",
    "\n",
    "    # our solution in each step - starting position\n",
    "    curr_coordinates = best_coordinates.copy()\n",
    "\n",
    "    # limit specifies the number of descents\n",
    "    \n",
    "    for k in range(limit):\n",
    "        gradient = gradient_approximation(fun, curr_coordinates, dim)\n",
    "        if gradient == -1: break\n",
    "\n",
    "        curr_coordinates = new_point(curr_coordinates, gradient)\n",
    "        curr_value = fun(curr_coordinates)[0]\n",
    "\n",
    "        if curr_value < best_value:\n",
    "            best_value = curr_value\n",
    "            best_coordinates = curr_coordinates.copy()\n",
    "            \n",
    "\n",
    "    return best_coordinates, best_value\n",
    "           \n",
    "# approximates all partial derivatives in the gradient            \n",
    "def gradient_approximation(function, x0, dim):\n",
    "    approx = []\n",
    "    x1 = x0.copy()\n",
    "\n",
    "    # approximation for each variable\n",
    "    for i in range(dim):\n",
    "        x1[i] = x0[i] + 0.01\n",
    "        y0 = function(x0)\n",
    "        y1 = function(x1)\n",
    "\n",
    "        # if the difference is too small we can \n",
    "        # increase the difference to move the point still\n",
    "        if math.isnan(y1 - y0): x1[i] = x1[i] + 0.04\n",
    "        approx.append(((y1 - y0)/(x1[i] - x0[i]))[0])\n",
    "        x1[i] = x0[i]\n",
    "    return approx\n",
    "\n",
    "# calculating the lambda and deciding on a new point\n",
    "def new_point(point, gradient):\n",
    "    new = []\n",
    "    for xi, grad in zip(point, gradient):\n",
    "\n",
    "        # calculating te number of digits in each derivative\n",
    "        no_digits = len(str(abs(math.floor(grad))))\n",
    "        alpha = 10**(-no_digits)\n",
    "        next_point = xi - alpha * grad\n",
    "\n",
    "        # checking the constraints\n",
    "        if next_point < upper_bound and next_point > lower_bound: \n",
    "            new.append(next_point)\n",
    "        else: new.append(xi)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                    TRUE: 21.1                 APPROXIMATE: 21.1                 DIFFERENCE: 0.0\n",
      "2                    TRUE: 26.91                APPROXIMATE: 39815.63             DIFFERENCE: 39788.72\n",
      "3                    TRUE: 311.6                APPROXIMATE: 1441.91              DIFFERENCE: 1130.31\n",
      "4                    TRUE: 311.6                APPROXIMATE: 1747.75              DIFFERENCE: 1436.15\n",
      "5                    TRUE: -48.47               APPROXIMATE: -23.91               DIFFERENCE: 24.56\n",
      "6                    TRUE: -91.36               APPROXIMATE: -29.92               DIFFERENCE: 61.44\n",
      "7                    TRUE: 32.49                APPROXIMATE: 2225.85              DIFFERENCE: 2193.36\n",
      "8                    TRUE: 71.6                 APPROXIMATE: 499.89               DIFFERENCE: 428.29\n",
      "9                    TRUE: -356.7               APPROXIMATE: -120.22              DIFFERENCE: 236.48\n",
      "10                   TRUE: 51.03                APPROXIMATE: 139871.12            DIFFERENCE: 139820.09\n",
      "11                   TRUE: -96.65               APPROXIMATE: 341.97               DIFFERENCE: 438.62\n",
      "12                   TRUE: 553.39               APPROXIMATE: 728246.51            DIFFERENCE: 727693.12\n",
      "13                   TRUE: 9.88                 APPROXIMATE: 288.43               DIFFERENCE: 278.55\n",
      "14                   TRUE: 405.47               APPROXIMATE: 405.49               DIFFERENCE: 0.02\n",
      "15                   TRUE: 64.25                APPROXIMATE: 575.29               DIFFERENCE: 511.04\n",
      "16                   TRUE: -43.28               APPROXIMATE: -9.14                DIFFERENCE: 34.14\n",
      "17                   TRUE: 227.51               APPROXIMATE: 229.34               DIFFERENCE: 1.83\n",
      "18                   TRUE: 227.51               APPROXIMATE: 253.28               DIFFERENCE: 25.77\n",
      "19                   TRUE: 73.06                APPROXIMATE: 90.73                DIFFERENCE: 17.67\n",
      "20                   TRUE: -123.81              APPROXIMATE: -122.66              DIFFERENCE: 1.15\n",
      "21                   TRUE: -44.42               APPROXIMATE: -43.72               DIFFERENCE: 0.7\n",
      "22                   TRUE: 222.1                APPROXIMATE: 224.16               DIFFERENCE: 2.06\n",
      "23                   TRUE: -1000.0              APPROXIMATE: -995.72              DIFFERENCE: 4.28\n",
      "24                   TRUE: -1.33                APPROXIMATE: 840.97               DIFFERENCE: 842.3\n",
      "OVERALL ABSOLUTE ERROR:  914970.6500000003\n"
     ]
    }
   ],
   "source": [
    "# testing our algorithm\n",
    "min_values_1 = [float('inf') for i in range(len(functions))]\n",
    "min_coordinates_1 = [[0 for i in range(40)] for k in range(len(functions))]\n",
    "no_runs = 100\n",
    "\n",
    "for i in range(no_runs):\n",
    "    for k in range(len(functions)):\n",
    "        curr_coordinates, curr_value = \\\n",
    "            gradient_descent(limit=300, dim=40, fun=functions[k+1])\n",
    "        if curr_value < min_values_1[k]:\n",
    "            min_values_1[k] = curr_value\n",
    "            min_coordinates_1[k] = curr_coordinates.copy()\n",
    "    \n",
    "comparison(min_values_1)\n",
    "results_to_file(min_coordinates_1, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking these results in to consideration we could deffenitly do better. We can see that in general this algorithm **does not perform well time-wise**, at least not given these parameters.\n",
    "While it does produce quite good results for some fuctions, we can see it fails most drastically in the case of **function 2**, **10** and **12**. Let us try a **local search** approach next to see if we could perhaps imporve these results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ALGORITHM 2:** Simulated Annealing\n",
    "\n",
    "This was the **second one** algorithm. The reason behind this one was the fact that these functions are very different, so the idea of some improved **random search** could work if we are trying to get the best possible result for all functions even though this could mean cutting some loses at those more specific ones. \n",
    "\n",
    "Now for a quick summery of how this algorithm works. We start with random points for each function. At every iteration of the algorithm take a **random step** between -0.1 and 0.1 for all 40 dimensions and check whether this **neighbour** produces a **better objective value**. Now in order not to get stuck in a **local minimum** we decide to move to this new point even if the solution is **worse** with some **probability** that is getting smaller with each iteration. This probability is regulated with a parameter called **``temperature``**. The higher the temperature the bigger the probability of accepting the bad solution, the more search space we explore. The lower the temperature, the more we focus on the solution at hand and optimizing this one. \n",
    "\n",
    "In our code **``temp``** is the initial temperature which is used to calculate the actual temperature in each step of the loop. The condition that decides the probability is taken from the Metropolis algorithm and goes as follows: \n",
    "1. first we calculate the difference between the current value and the candidate value of the neighbour: $\\bigtriangleup$\n",
    "2. if the difference is a negative one then the solution is better and we accept and save it - this is now our current position\n",
    "3. otherwise we accept the probability with the Boltzmann distribution: $P = e^{\\cfrac {\\bigtriangleup} {temperature}}$\n",
    "4. we generate a random number and if the number is higher we move to the candidate point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation\n",
    "def simulated_annealing(limit, dim, temp, fun):\n",
    "    \n",
    "    # setting the best solution to the first one we try and updating it later on\n",
    "    best_coordinates = np.random.uniform(lower_bound, upper_bound, dim)\n",
    "    best_value = fun(best_coordinates)[0]\n",
    "\n",
    "    # our solution in each step - starting position\n",
    "    curr_coordinates = best_coordinates.copy()\n",
    "    curr_value = best_value\n",
    "    \n",
    "    \n",
    "    for k in range(limit):\n",
    "\n",
    "        # calculating the neighbour candidate and checking the bounds\n",
    "        candidate = curr_coordinates + np.random.uniform(-0.1, 0.1, 40)\n",
    "        if all(xi < upper_bound and xi > lower_bound for xi in candidate):\n",
    "            \n",
    "            # if the value is better than the best one so far we save it\n",
    "            candidate_value = fun(candidate)[0]\n",
    "            if candidate_value < best_value:\n",
    "                best_coordinates = candidate.copy()\n",
    "                best_value = candidate_value\n",
    "            \n",
    "            # calculating the difference between the values and the new temperature\n",
    "            diff = candidate_value - curr_value\n",
    "            temperature = temp / float(k + 1)\n",
    "\n",
    "            # calculating the probability of accepting a bad solution\n",
    "            x = -diff / temperature\n",
    "            try: condition = math.exp(x)\n",
    "            except OverflowError: condition = float(\"Inf\")\n",
    "\n",
    "            # random throw whether we accept the solution\n",
    "            if diff < 0 or np.random.random() < condition:\n",
    "                curr_coordinates = candidate.copy()\n",
    "                curr_value = candidate_value\n",
    "            \n",
    "    return [best_coordinates, best_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                    TRUE: 21.1                 APPROXIMATE: 21.39                DIFFERENCE: 0.29\n",
      "2                    TRUE: 26.91                APPROXIMATE: 25920.63             DIFFERENCE: 25893.72\n",
      "3                    TRUE: 311.6                APPROXIMATE: 1256.07              DIFFERENCE: 944.47\n",
      "4                    TRUE: 311.6                APPROXIMATE: 1330.9               DIFFERENCE: 1019.3\n",
      "5                    TRUE: -48.47               APPROXIMATE: 168.67               DIFFERENCE: 217.14\n",
      "6                    TRUE: -91.36               APPROXIMATE: -67.81               DIFFERENCE: 23.55\n",
      "7                    TRUE: 32.49                APPROXIMATE: 433.45               DIFFERENCE: 400.96\n",
      "8                    TRUE: 71.6                 APPROXIMATE: 122.27               DIFFERENCE: 50.67\n",
      "9                    TRUE: -356.7               APPROXIMATE: -310.42              DIFFERENCE: 46.28\n",
      "10                   TRUE: 51.03                APPROXIMATE: 26798.82             DIFFERENCE: 26747.79\n",
      "11                   TRUE: -96.65               APPROXIMATE: 160.02               DIFFERENCE: 256.67\n",
      "12                   TRUE: 553.39               APPROXIMATE: 76399.54             DIFFERENCE: 75846.15\n",
      "13                   TRUE: 9.88                 APPROXIMATE: 74.85                DIFFERENCE: 64.97\n",
      "14                   TRUE: 405.47               APPROXIMATE: 405.69               DIFFERENCE: 0.22\n",
      "15                   TRUE: 64.25                APPROXIMATE: 1246.38              DIFFERENCE: 1182.13\n",
      "16                   TRUE: -43.28               APPROXIMATE: -31.87               DIFFERENCE: 11.41\n",
      "17                   TRUE: 227.51               APPROXIMATE: 238.0                DIFFERENCE: 10.49\n",
      "18                   TRUE: 227.51               APPROXIMATE: 262.26               DIFFERENCE: 34.75\n",
      "19                   TRUE: 73.06                APPROXIMATE: 91.88                DIFFERENCE: 18.82\n",
      "20                   TRUE: -123.81              APPROXIMATE: -121.81              DIFFERENCE: 2.0\n",
      "21                   TRUE: -44.42               APPROXIMATE: -44.3                DIFFERENCE: 0.12\n",
      "22                   TRUE: 222.1                APPROXIMATE: 224.12               DIFFERENCE: 2.02\n",
      "23                   TRUE: -1000.0              APPROXIMATE: -997.65              DIFFERENCE: 2.35\n",
      "24                   TRUE: -1.33                APPROXIMATE: 1041.31              DIFFERENCE: 1042.64\n",
      "OVERALL ABSOLUTE ERROR:  133818.91\n"
     ]
    }
   ],
   "source": [
    "# testing our algorithm\n",
    "min_values_2 = [float('inf') for i in range(len(functions))]\n",
    "min_coordinates_2 = [[0 for i in range(40)] for k in range(len(functions))]\n",
    "no_runs = 300\n",
    "\n",
    "for i in range(no_runs):\n",
    "    for k in range(len(functions)):\n",
    "        curr_coordinates, curr_value =\\\n",
    "            simulated_annealing(limit=3000, dim=40, temp=100, fun=functions[k+1])\n",
    "        if curr_value < min_values_2[k]:\n",
    "            min_values_2[k] = curr_value\n",
    "            min_coordinates_2[k] = curr_coordinates.copy()\n",
    "            \n",
    "\n",
    "comparison(min_values_2)\n",
    "results_to_file(min_coordinates_2, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is **overall better**, if we take the absolute error as a measurment of succsess, but gradient descent performed better for a few **specific cases**. The difference in the result is mainly due to different results in the case of **function 12** and **10**. These were two simple algorithms, now on to a pair of more complicated ones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ALGORITHM 3:** Covariance Matrix Adaptation Evolution Strategy\n",
    "Considering none of the algorithms come close to all of the actual minimums, we decided to look for any **academic articales** on these functions and found the paper titled \n",
    "</br>**[Comparing Results of 31 Algorithms from the Black-Box Optimization Benchmarking BBOB](https://www.researchgate.net/publication/220740712_Comparing_Results_of_31_Algorithms_from_the_Black-Box_Optimization_Benchmarking_BBOB-2009#fullTextFileContent)**. It seemed that the best result for functions in **40 dimensions** was the **BI-population CMA-ES algorithm**, so we looked it up and decided to implement our own version of a **[CMA-ES](https://arxiv.org/pdf/1604.00772.pdf)** algorithm. This is an **evolutionary algorithm** which is an approach we have not yet tried, so perhaps this will lead to a better solution. It's basis relies on well known evolutionary concepts, mainly the importance of mutation for providing more **variety** to the search and **selection** where only the better offsprings remain.\n",
    "\n",
    "The actual implementation of the algorithm is quite complicated but let us try and recap the important details and the main idea behind this strategy. In the CMA Evolution Strategy, a population of new search points often called offsprings is generated by **sampling** a **multivariate normal distribution**, which is the generalization of the normal distribution in higher dimensions. It is uniquely determined by the **mean** **$m$** and the positive definite **covariance matrix $C$**. The **``sigma``** in our code again represents the step size in this sampling method.\n",
    "\n",
    "This would be the basic formula for finding **next generation offsprings**:\n",
    "${x_k^{(g+1)}} = m^{(g)} + {\\sigma}^{(g)} \\cdot N(0, C^{(g)})$\n",
    "- $C$ is the covariance matrix at generation $g$\n",
    "- $m$ is the mean value of the search distribution at generation $g$\n",
    "- $\\sigma$ is the overall standard deviation represented with our **``sigma``**\n",
    "- $k$ is in the range of $\\lambda$ of our population size, the number of offsprings: **``offsprings``**\n",
    "- we sort the $x_k$ elements based on their objective value $f(x_k)$ from the smallest value up\n",
    "\n",
    "The normal distribution defined by the covariance matrix can be written in many ways. We will use this to our advantage when implementing the algorithm: $N(m, C) ∼ m + C^{\\frac{1}{2}}\\cdot N(0,I)$\n",
    "</br> The following equality also holds: $C = B D^{2} B^T$, where $B$ are the **eigenvectors** and $D$ is the **diagonal matrix** with **eigenvalues** of $C$ on the diagonal.\n",
    "</br> The reason that the **eigendecoposition** surely exists is in the positive definite trait of the matrix $C$\n",
    "</br> Through obvious matrix traits known from linear algebra we can see that $C^{\\frac{1}{2}} = BD^{-1}B^T$\n",
    "\n",
    "Now that we have the offsprings we have to calculate the new **mean**. This is the weighted average of $\\mu$ selected points from our sample. ${m^{(g+1)}} = m^{(g)} + \\sum_{i=1}^{\\mu}{w_i \\cdot x_{i:\\lambda}^{(g+1)}}$\n",
    "</br> The $w_i$ symbol represents the weights in our vector **``weights``** for recombination and $\\mu$ is the parent population size **``parents``**.\n",
    "\n",
    "**``parents_eff``** or ${\\mu}_{eff}$ represents the **variance effective selection mass** for reasonble setting of weights. We also track 2 different **evolution paths**, which store information about the changes made to the mean vector in each generation of the algorithm. \n",
    "\n",
    "We express one as $p_c^g$ and it tells us the cumulative sum of all steps up to generation $g$. In practice we use exponential soothing for this step. \n",
    "</br>$p_{c}^{(g+1)} = (1 - c_{c}) p_{c}^{(g)} + h_{\\sigma} \\cdot \\sqrt{c_{c}(2 - c_{c}){\\mu}_{eff}} \\cdot \\frac{m^{(g+1)} - m^{(g)}}{\\sigma}$\n",
    "\n",
    "We also use **step size control** to regulate the overall scale of the distribution. For this the second evolution path is used, marked as $p_{\\sigma}^g$. If the single steps cancel each other out, step size is too large and should be decreased. If the steps are pointing to a similar direction we can correct the step size to a larger number to cover these simlar steps in less steps. The optimal situation is having **uncorrelated steps**.\n",
    "</br>$p_{\\sigma}^{(g+1)} = (1 - c_{\\sigma}) p_{\\sigma}^{(g)} + \\sqrt{c_{\\sigma}(2 - c_{\\sigma}){\\mu}_{eff}} \\cdot C^{(g)^{\\frac{1}{2}}} \\cdot \\frac{m^{(g+1)} - m^{(g)}}{\\sigma}$\n",
    "\n",
    "Then we update the covariance matrix using this value.\n",
    "</br>$C^{(g+1)} = (1 - \\text{ learning rate} - c_{\\mu}) \\cdot C^{(g)} + \\text{ learning rate} \\cdot p_c \\cdot p_c^T + c_{\\mu} \\sum_{i=1}^{\\mu}{w_i \\cdot \\frac{x_{i:\\lambda} - m}{\\sigma} \\cdot (\\frac{x_{i:\\lambda} - m}{\\sigma})^T}$\n",
    "\n",
    "$h_{\\sigma}$ is the Heaviside function which stalls the update of the $p_c$ path in necessary circumstances.\n",
    "\n",
    "With all this theoretical background, one can understand the algorithm bellow. First we set the parent and offspring population size, the weights step size control **``cs``**, covariance matrix adaptation **``cc``** and the **``learning_rate``**. The evolution paths **``ps``** and **``pc``** must be set to zero and **``C``** is set to I. Then we chose the starting mean **``mean``** and the step size **``sigma``**. At this point we start iterating the procedure until the termination condition is met. We then semple a new population of search points, do the selection and recombination, watch the setp size control and do the covariance matrix adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation\n",
    "def cma_es(limit, dim, fun, stopfitness):\n",
    "\n",
    "    # initialization\n",
    "    mean = np.random.rand(dim,1) # starting mean\n",
    "    sigma = 0.3 # step size\n",
    "\n",
    "    # selection parameters\n",
    "    offsprings = 4 + math.floor(3 * math.log(dim)) # size of the offspring population \n",
    "    parents = offsprings / 2 # size of the parent population\n",
    "    weights = np.log(parents + 1/2) - np.log(np.arange(1, math.floor(parents) + 1))\n",
    "    weights = np.atleast_2d(weights / np.sum(weights)).T # normalization\n",
    "    parents = math.floor(parents)\n",
    "    parents_eff = np.sum(weights)**2 / np.sum(weights**2)\n",
    "    \n",
    "\n",
    "    # adaptation parameters\n",
    "    cc = (4 + parents_eff / dim) / ((dim + 4) + (2 * parents_eff / dim)) \n",
    "    cs = (parents_eff + 2) / (dim + parents_eff + 5) \n",
    "    learning_rate = 2 / ((dim + 1.3)**2 + parents_eff)\n",
    "    cmu_temp = 2 * (parents_eff - 2 + 1/parents_eff) / ((dim + 2)**2 + parents_eff)\n",
    "    cmu = min(1 - learning_rate, cmu_temp)\n",
    "    damps = 1 + 2 * max(0, math.sqrt((parents_eff - 1) / (dim + 1)) - 1) + cs\n",
    "\n",
    "    # setting initial generation 0 paths and matrices\n",
    "    pc = np.zeros((dim,1))\n",
    "    ps = np.zeros((dim,1))\n",
    "    B = np.eye(dim)\n",
    "    D = np.ones((dim,1))\n",
    "    C = np.matmul(np.matmul(B, np.diag((D**2).flatten())), B.T)\n",
    "    invsqrtC = np.matmul(np.matmul(B, (np.diag((D**-1).flatten()))), B.T)\n",
    "    eigeneval = 0\n",
    "    chi_dim = dim**0.5 * (1 - 1/(4 * dim) + 1/(21* dim**2))\n",
    "\n",
    "    # iterations\n",
    "    function_calls = 0\n",
    "    coordinates = np.zeros((dim, offsprings))\n",
    "    value = np.zeros(offsprings)\n",
    "    \n",
    "    while(function_calls < limit):\n",
    "\n",
    "        # generating and evaluating lambda offsprings\n",
    "        for i in range(offsprings):\n",
    "            coordinates[:, i] = \\\n",
    "                (mean + sigma * np.matmul(B, D * np.random.normal())).flatten()\n",
    "\n",
    "            # checking constraints and repairing coordinates\n",
    "            for k in np.nonzero([coordinates[:, i] > upper_bound]):\n",
    "                coordinates[k, i] = upper_bound\n",
    "            for k in np.nonzero([coordinates[:, i] < lower_bound]):\n",
    "                coordinates[k, i] = lower_bound\n",
    "\n",
    "            value[i] = fun(coordinates[:, i])\n",
    "            function_calls = function_calls + 1\n",
    "        \n",
    "        # sort by values and compute weighted mean\n",
    "        indices = np.argsort(value)\n",
    "        value = np.sort(value)\n",
    "\n",
    "        old_mean = mean\n",
    "        mean = np.matmul(coordinates[:, indices[:parents]], weights)\n",
    "    \n",
    "        # update evolution paths\n",
    "        ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * parents_eff) \n",
    "        ps = ps * np.matmul(invsqrtC, ((mean - old_mean)) / sigma)\n",
    "        cond = np.sqrt(1 - (1 - cs)**(2 * function_calls / offsprings)) / chi_dim\n",
    "        cond = np.linalg.norm(ps) / cond\n",
    "        hsig = cond < 1.4 + 2/(dim + 1)\n",
    "        pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * parents_eff) \n",
    "        pc = pc * ((mean - old_mean) / sigma)\n",
    "        \n",
    "        # adapt covariance matrix C\n",
    "        repeat_matrix = np.matlib.repmat(old_mean, 1, parents)\n",
    "        temp = (1 / sigma) * (coordinates[:, indices[:parents]] - repeat_matrix) \n",
    "        C = (1 - learning_rate - cmu) * C \n",
    "        C = C + learning_rate * (np.matmul(pc, pc.T) + (1 - hsig) * cc * (2 - cc) * C)\n",
    "        C = C + cmu * np.matmul(np.matmul(temp, np.diag(weights.flatten())), temp.T)\n",
    "\n",
    "        # adapt step size sigma based on calculated ps\n",
    "        sigma = sigma * math.exp((cs / damps) * (np.linalg.norm(ps) / chi_dim - 1)); \n",
    "        \n",
    "        # decomposition of C for the next iteration\n",
    "        if function_calls - eigeneval > offsprings / (learning_rate + cmu) / dim /10:\n",
    "            eigeneval = function_calls # time optimization\n",
    "            C = np.triu(C,0) + np.triu(C,1).T # for symmetry\n",
    "            D, B = np.linalg.eig(C) \n",
    "            D = np.real(D) # to avert complex matrix warning\n",
    "            B = np.real(B)\n",
    "            D = np.sqrt(D)\n",
    "            invsqrtC = np.matmul(np.matmul(B, (np.diag(D**-1))), B.T)\n",
    "            D = np.atleast_2d(D).T\n",
    "        \n",
    "        # termination condition\n",
    "        if value[0] <= stopfitness + 0.01:\n",
    "            break\n",
    "\n",
    "    return coordinates[:, indices[0]], fun(coordinates[:, indices[0]])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                    TRUE: 21.1                 APPROXIMATE: 21.11                DIFFERENCE: 0.01\n",
      "2                    TRUE: 26.91                APPROXIMATE: 1063.63              DIFFERENCE: 1036.72\n",
      "3                    TRUE: 311.6                APPROXIMATE: 445.92               DIFFERENCE: 134.32\n",
      "4                    TRUE: 311.6                APPROXIMATE: 694.65               DIFFERENCE: 383.05\n",
      "5                    TRUE: -48.47               APPROXIMATE: -40.03               DIFFERENCE: 8.44\n",
      "6                    TRUE: -91.36               APPROXIMATE: -90.45               DIFFERENCE: 0.91\n",
      "7                    TRUE: 32.49                APPROXIMATE: 99.46                DIFFERENCE: 66.97\n",
      "8                    TRUE: 71.6                 APPROXIMATE: 100.35               DIFFERENCE: 28.75\n",
      "9                    TRUE: -356.7               APPROXIMATE: -349.91              DIFFERENCE: 6.79\n",
      "10                   TRUE: 51.03                APPROXIMATE: 1058.87              DIFFERENCE: 1007.84\n",
      "11                   TRUE: -96.65               APPROXIMATE: 32.13                DIFFERENCE: 128.78\n",
      "12                   TRUE: 553.39               APPROXIMATE: 553.47               DIFFERENCE: 0.08\n",
      "13                   TRUE: 9.88                 APPROXIMATE: 9.9                  DIFFERENCE: 0.02\n",
      "14                   TRUE: 405.47               APPROXIMATE: 405.48               DIFFERENCE: 0.01\n",
      "15                   TRUE: 64.25                APPROXIMATE: 206.53               DIFFERENCE: 142.28\n",
      "16                   TRUE: -43.28               APPROXIMATE: -34.93               DIFFERENCE: 8.35\n",
      "17                   TRUE: 227.51               APPROXIMATE: 229.88               DIFFERENCE: 2.37\n",
      "18                   TRUE: 227.51               APPROXIMATE: 232.09               DIFFERENCE: 4.58\n",
      "19                   TRUE: 73.06                APPROXIMATE: 73.99                DIFFERENCE: 0.93\n",
      "20                   TRUE: -123.81              APPROXIMATE: -122.25              DIFFERENCE: 1.56\n",
      "21                   TRUE: -44.42               APPROXIMATE: -38.17               DIFFERENCE: 6.25\n",
      "22                   TRUE: 222.1                APPROXIMATE: 224.69               DIFFERENCE: 2.59\n",
      "23                   TRUE: -1000.0              APPROXIMATE: -998.56              DIFFERENCE: 1.44\n",
      "24                   TRUE: -1.33                APPROXIMATE: 58.71                DIFFERENCE: 60.04\n",
      "OVERALL ABSOLUTE ERROR:  3033.0800000000004\n"
     ]
    }
   ],
   "source": [
    "# testing our algorithm\n",
    "min_values_3 = [float('inf') for i in range(len(functions))]\n",
    "min_coordinates_3 = [[0 for i in range(40)] for k in range(len(functions))]\n",
    "no_calls = 10\n",
    "\n",
    "for k in range(no_calls):\n",
    "    for i in range(len(functions)):\n",
    "        curr_coordinates, curr_value = \\\n",
    "            cma_es(100000, 40, functions[i+1], true_minimums[i+1])\n",
    "        if curr_value < min_values_3[i]:\n",
    "                min_values_3[i] = curr_value\n",
    "                min_coordinates_3[i] = curr_coordinates.copy()\n",
    "\n",
    "comparison(min_values_3)\n",
    "results_to_file(min_coordinates_3, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ALGORITHM 4:** Nelder-Mead method\n",
    "This fourth alogirthm is a numerical method used to find the minimum of an objective function, in our case BBOB functions, in a multidimensional space.\n",
    "The method uses the concept of a **simplex**, which is a special **polytope** *(a geometric object with flat sides)* of $n+1$ vertices in n dimensions *(for example on a plane this would be a triangle)*. \n",
    "</br>Now for the quick summary of how this algorithm works. As the initial point we take a point of all **zeros** and generate others with a fixed step along each dimension in turn. Then we start the simplex iteration. We are trying to minimize function $f(x)$, where $x \\in \\mathbb{R}$. Our current points are $ x_1, …, x_{n+1}$.\n",
    "\n",
    "1. **ORDER:**  Order according to the values at the vertices: $f(x_1) \\leq … \\leq f(x_{n+1})$ and check if the method should stop.\n",
    "\n",
    "2. **CENTROID:** Calculate the centroid $x_0$ of all points except $x_{n+1}$.\n",
    "\n",
    "3. **REFLECTION:** Compute reflected point $x_r = x_0 + \\alpha(x_0 – x_{n+1})$, for $\\alpha = 1$. \n",
    "</br>If the reflected point is better than the second worst, but not better than the best, $f(x_1) \\leq f(x_r) \\lt f(x_n)$, \n",
    "</br>than obtain a new simplex by replacing the worst point $x_{n+1}$ with the reflected point $x_r$, and go to step 1.\n",
    "\n",
    "4. **EXPANSION:** If the reflected point is the best point so far, $f(x_r) \\lt f(x_1)$, then compute the expanded point $x_e = x_0 + \\gamma(x_r – x_0)$ with $\\gamma = 2$. \n",
    "</br>If the expanded point is better than the reflected point, $f(x_e) \\lt f(x_r)$, then obtain a new simplex by replacing the worst $x_{n+1}$ with the expanded point $x_e$ and go to step 1; \n",
    "</br>else obtain a new simplex by replacing the worst point $x_{n+1}$ with the reflected point $x_r$ and go to step 1.\n",
    "\n",
    "5. **CONTRACTION:** Here it is certain that $f(x_r) \\geq f(x_n)$, where $x_n$ is second to the worst point. \n",
    "</br>\n",
    "</br>If $f(x_r) \\lt f(x_{n+1})$, then compute the contracted point on the outside $x_c = x_0 + \\rho(x_r – x_0)$ with $\\rho=0.5$. \n",
    "</br>If the contracted point is better than the reflected point, $f(x_c) \\lt f(x_r)$, then obtain a new simplex by replacing the worst point $x_{n+1}$ with the contracted point $x_c$ and go to step 1.\n",
    "</br>Else go to step 6. \n",
    "</br>\n",
    "</br>If $f(x_r) \\geq f(x_n+1)$, then compute the contracted point on the inside $x_c = x_0 + \\rho*(x_{n+1} – x_0)$ with $\\rho=0.5$. \n",
    "</br>If the contracted point is better than the worst point, $f(x_c) \\lt f(x_{n+1})$, then obtain a new simplex by replacing the worst point xn+1 with the contracted point $x_c$ and go to step 1. \n",
    "</br>Else go to step 6.\n",
    "\n",
    "6. **SHRINK:** Replace all points except the best ($x_1$) with $x_i = x_1 + \\sigma(x_i – x_1)$ and go to step 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation\n",
    "def nelder_mead(limit, dim, f, tolerance=10e-9, how_many_times_same_result=500, step=1):\n",
    "    best_coordinates = np.zeros(dim)\n",
    "    best_value = f(best_coordinates)\n",
    "\n",
    "    curr_result = []\n",
    "    curr_result.append([best_value, best_coordinates])\n",
    "\n",
    "    for i in range(dim):\n",
    "        curr = best_coordinates.copy()\n",
    "        curr[i] = curr[i] + step\n",
    "        curr_result.append([f(curr), curr])\n",
    "\n",
    "    for k in range(limit):\n",
    "        # order\n",
    "        curr_result.sort(key=lambda x: x[0])\n",
    "        best = curr_result[0][0]\n",
    "\n",
    "        if best >= best_value - tolerance:\n",
    "            how_many_times_same_result -= 1\n",
    "        else:\n",
    "            how_many_times_same_result = 500\n",
    "            best_value = best\n",
    "\n",
    "        if (how_many_times_same_result <= 0): \n",
    "            return curr_result[0][1], curr_result[0][0][0]\n",
    "\n",
    "        # centroid\n",
    "        cent = centroid(dim, curr_result)\n",
    "\n",
    "        # reflection\n",
    "        ref, check1 = reflection(cent, curr_result, f)\n",
    "        if (check1): continue\n",
    "\n",
    "        # expansion\n",
    "        check2 = expansion(cent, ref, curr_result, f)\n",
    "        if (check2): continue\n",
    "\n",
    "        # contraction\n",
    "        check3 = contraction(cent, ref, curr_result, f)\n",
    "        if (check3): continue\n",
    "\n",
    "        # shrink\n",
    "        curr_result = shrink(curr_result, f)\n",
    "        curr_result[0][1] = constraints(curr_result[0][1])\n",
    "\n",
    "    return curr_result[0][1], curr_result[0][0][0]\n",
    "    \n",
    "def centroid(dim, curr_result):\n",
    "    cent = np.zeros(dim)\n",
    "    for el in curr_result[:-1]:\n",
    "        for index, number in enumerate(el[1]):\n",
    "            cent[index] = cent[index] + number / (len(curr_result)-1)\n",
    "    return cent\n",
    "\n",
    "def reflection(c, curr_result, f, alpha=1):\n",
    "    r = c + alpha*(c - curr_result[-1][1])\n",
    "    value_r = f(r)\n",
    "    check = False\n",
    "\n",
    "    if curr_result[0][0] <= value_r < curr_result[-2][0]:\n",
    "        del curr_result[-1]\n",
    "        curr_result.append([value_r, r])\n",
    "        check = True\n",
    "\n",
    "    return r, check\n",
    "\n",
    "\n",
    "def expansion(c, r, curr_result, f, gamma=2):\n",
    "    value_r = f(r)\n",
    "    check = False\n",
    "\n",
    "    if value_r < curr_result[0][0]:\n",
    "        e = c + gamma*(r - c)\n",
    "        value_e = f(e)\n",
    "\n",
    "        if value_e < value_r:\n",
    "            del curr_result[-1]\n",
    "            curr_result.append([value_e, e])\n",
    "        elif value_e >= value_r:\n",
    "            del curr_result[-1]\n",
    "            curr_result.append([value_r, r])\n",
    "        check = True\n",
    "\n",
    "    return check\n",
    "\n",
    "\n",
    "def contraction(c, r, curr_result, f, rho=0.5):\n",
    "    check = False\n",
    "    value_r = f(r)\n",
    "\n",
    "    if value_r < curr_result[-1][0]:\n",
    "        cont = c + rho*(r - c)\n",
    "        value_cont = f(cont)\n",
    "        if value_cont < value_r:\n",
    "            del curr_result[-1]\n",
    "            curr_result.append([value_cont, cont])\n",
    "            check = True\n",
    "    else:\n",
    "        cont = c + rho*(curr_result[-1][1] - c)\n",
    "        value_cont = f(cont)\n",
    "        if value_cont < curr_result[-1][0]:\n",
    "            del curr_result[-1]\n",
    "            curr_result.append([value_cont, cont])\n",
    "            check = True\n",
    "\n",
    "    return check\n",
    "\n",
    "\n",
    "def shrink(curr_result, f, sigma=0.5):\n",
    "    result = []\n",
    "\n",
    "    for r in curr_result:\n",
    "        x = curr_result[0][1] + sigma*(r[1] - curr_result[0][1])\n",
    "        result.append([f(x), x])\n",
    "\n",
    "    return result\n",
    "\n",
    "def constraints(coordinates):\n",
    "    for k in np.nonzero([coordinates > upper_bound]):\n",
    "            coordinates[k] = upper_bound\n",
    "    for k in np.nonzero([coordinates < lower_bound]):\n",
    "        coordinates[k] = lower_bound\n",
    "    return coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                    TRUE: 21.1                 APPROXIMATE: 21.1                 DIFFERENCE: 0.0\n",
      "2                    TRUE: 26.91                APPROXIMATE: 21727.95             DIFFERENCE: 21701.04\n",
      "3                    TRUE: 311.6                APPROXIMATE: 1157.3               DIFFERENCE: 845.7\n",
      "4                    TRUE: 311.6                APPROXIMATE: 846.91               DIFFERENCE: 535.31\n",
      "5                    TRUE: -48.47               APPROXIMATE: -48.47               DIFFERENCE: 0.0\n",
      "6                    TRUE: -91.36               APPROXIMATE: 92.68                DIFFERENCE: 184.04\n",
      "7                    TRUE: 32.49                APPROXIMATE: 189.3                DIFFERENCE: 156.81\n",
      "8                    TRUE: 71.6                 APPROXIMATE: 145.33               DIFFERENCE: 73.73\n",
      "9                    TRUE: -356.7               APPROXIMATE: -324.23              DIFFERENCE: 32.47\n",
      "10                   TRUE: 51.03                APPROXIMATE: 2583.39              DIFFERENCE: 2532.36\n",
      "11                   TRUE: -96.65               APPROXIMATE: -16.14               DIFFERENCE: 80.51\n",
      "12                   TRUE: 553.39               APPROXIMATE: 554.76               DIFFERENCE: 1.37\n",
      "13                   TRUE: 9.88                 APPROXIMATE: 18.3                 DIFFERENCE: 8.42\n",
      "14                   TRUE: 405.47               APPROXIMATE: 405.48               DIFFERENCE: 0.01\n",
      "15                   TRUE: 64.25                APPROXIMATE: 1169.91              DIFFERENCE: 1105.66\n",
      "16                   TRUE: -43.28               APPROXIMATE: -17.1                DIFFERENCE: 26.18\n",
      "17                   TRUE: 227.51               APPROXIMATE: 241.31               DIFFERENCE: 13.8\n",
      "18                   TRUE: 227.51               APPROXIMATE: 281.04               DIFFERENCE: 53.53\n",
      "19                   TRUE: 73.06                APPROXIMATE: 73.08                DIFFERENCE: 0.02\n",
      "20                   TRUE: -123.81              APPROXIMATE: -121.28              DIFFERENCE: 2.53\n",
      "21                   TRUE: -44.42               APPROXIMATE: -38.17               DIFFERENCE: 6.25\n",
      "22                   TRUE: 222.1                APPROXIMATE: 224.69               DIFFERENCE: 2.59\n",
      "23                   TRUE: -1000.0              APPROXIMATE: -997.62              DIFFERENCE: 2.38\n",
      "24                   TRUE: -1.33                APPROXIMATE: 240.05               DIFFERENCE: 241.38\n",
      "OVERALL ABSOLUTE ERROR:  27606.09\n"
     ]
    }
   ],
   "source": [
    "# testing the code\n",
    "min_values_4 = [float('inf') for i in range(len(functions))]\n",
    "min_coordinates_4 = [[0 for i in range(40)] for k in range(len(functions))]\n",
    "no_runs = 1 \n",
    "\n",
    "dim = 40\n",
    "limit = 1000000\n",
    "\n",
    "for i in range(no_runs):\n",
    "    for k in range(len(functions)):\n",
    "        curr_coordinates, curr_value = nelder_mead(limit, dim, functions[k+1])\n",
    "        if curr_value < min_values_4[k]:\n",
    "            min_values_4[k] = curr_value\n",
    "            min_coordinates_4[k] = curr_coordinates.copy()\n",
    "    \n",
    "comparison(min_values_4)\n",
    "results_to_file(min_coordinates_4, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### **Results**\n",
    "\n",
    "In conclusion the best results overall were with the **CMA-ES** algorithm but the last one did pretty good as well. Finally let us present the solutions. \n",
    "\n",
    "| Algorithm           | Function 1 | Function 2 | Function 3 | Function 4 | Function 5 |\n",
    "|---------------------|------------|------------|------------|------------|------------|\n",
    "| Global minimum      | 21.1       | 26.91      | 311.6      | 311.6      | -48.47     |\n",
    "| Gradient Descent    | 21.1       | 39815.63   | 1441.91    | 1747.75    | -23.91     |\n",
    "| Simulated annealing | 21.39      | 25920.63   | 1256.07    | 1330.9     | 168.67     | \n",
    "| CMA-ES              | 21.11      | 1063.63    | 445.92     | 694.65     | -40.03     | \n",
    "| Nelder-Mead         | 21.1       | 21727.95   | 1157.3     | 846.91     | -48.47     | \n",
    "\n",
    "</br>\n",
    "\n",
    "| Algorithm           | Function 6 | Function 7| Function 8 | Function 9 | Function 10 | \n",
    "|---------------------|------------|-----------|------------|------------|-------------|\n",
    "| Global minimum      | -91.36     | 32.49     | 71.6       | -356.7     | 51.03       |\n",
    "| Gradient Descent    | -29.92     | 2225.85   | 499.89     | -120.22    | 139871.12   |\n",
    "| Simulated annealing | -67.81     | 433.45    | 122.27     | -310.42    | 26798.82    |\n",
    "| CMA-ES              | -90.45     | 99.46     | 100.35     | -349.91    | 1058.87     |\n",
    "| Nelder-Mead         | 92.68      | 189.3     | 145.33     | -324.23    | 2583.39     |\n",
    "\n",
    "</br>\n",
    "\n",
    "| Algorithm           |Function 11  |Function 12 |Function 13 |Function 14 |Function 15 |\n",
    "|---------------------|------------ |------------|------------|------------|------------|\n",
    "| Global minimum      | -96.65      | 553.39      | 9.88        | 405.47      | 64.25       |\n",
    "| Gradient Descent    | 341.97      | 728246.51  | 288.43     | 405.49     | 575.29     |\n",
    "| Simulated annealing | 160.02      | 76399.54   | 74.85      | 405.69     | 1246.38    |\n",
    "| CMA-ES              | 32.13       | 553.47     | 9.9        | 405.48     | 206.53     |\n",
    "| Nelder-Mead         | -16.14      | 554.76     | 18.3       | 405.48     | 1169.91    |\n",
    "\n",
    "</br>\n",
    "\n",
    "| Algorithm           | Function 16 | Function 17 | Function 18 | Function 19 | Function 20 |\n",
    "|---------------------|-------------|-------------|-------------|-------------|-------------|\n",
    "| Global minimum      | -43.28      | 227.51      | 227.51      | 73.06       | -123.81     |\n",
    "| Gradient Descent    | -9.14       | 229.34      | 253.28      | 90.73       | -122.66     |\n",
    "| Simulated annealing | -31.87      | 238         | 262.26      | 91.88       | -121.81     |\n",
    "| CMA-ES              | -34.93      | 229.88      | 232.09      | 73.99       | -122.25     |\n",
    "| Nelder-Mead         | -17.1       | 241.31      | 281.04      | 73.08       | -121.28     |\n",
    "\n",
    "</br>\n",
    "\n",
    "| Algorithm           | Function 21 | Function 22 | Function 23 | Function 24 |\n",
    "|---------------------|-------------|-------------|-------------|-------------|\n",
    "| Global minimum      | -44.42      | 222.1       | -1000       | -1.33       |\n",
    "| Gradient Descent    | -43.72      | 224.16      | -995.72     | 840.97      |\n",
    "| Simulated annealing | -44.3       | 224.12      | -997.65     | 1041.31     |\n",
    "| CMA-ES              | -38.17      | 224.69      | -998.56     | 58.71       |\n",
    "| Nelder-Mead         | -38.17      | 224.69      | -997.62     | 240.05      |\n",
    "\n",
    "<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

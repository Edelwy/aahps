{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AAHPS: ASSIGNMENT 5**\n",
    "\n",
    "**Authors**: Nina Mislej and Nika Molan\n",
    "<br/>**Student numbers**: 63200016 and 63200017\n",
    "\n",
    "### **Exercise Description**\n",
    "The aim of this assignment is to find the best result for **24 optimization** (*minimization*) **functions** that are available as **BBOB** (*Black-Box Optimization Benchmark*). We will implement **2 optimization programs** one of which has to be **local search** and the other one can be **any** optimization approach.\n",
    "<br/>The functions are available in ***smoof*** package in **R** and we will initialize the functions with **40 dimensions** using the *iid*: **2023**\n",
    "\n",
    "In addition to this report the results include the **coordinates** for each of the **24 minimums**. These are included in a separate file, one for each algorithm. \n",
    "<br/>One line represents **40**-**touple** for one function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Functions Description**\n",
    "\n",
    "The [functions](https://numbbo.github.io/gforge/downloads/download16.00/bbobdocfunctions.pdf) are designed to cover a lot of different problems that occur with **different optimization approaches**. Some of the functions are meant to test if the algorithm gets **stuck in local optimums**, some check if the algorithm can find optimums **bordering** the function domain, some functions are symetric while some highly asymetric and so on and so forth. We have to take this properties in to account when chosing our algorithms to optimize the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the package and enviroment\n",
    "import numpy as np\n",
    "import math\n",
    "from rpy2.robjects import numpy2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "numpy2ri.activate() # automatic conversion from numpy to R arrays\n",
    "smoof = importr(\"smoof\") # importing R smoof package"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we **initialize** the functions.\n",
    "</br>In this process we also save their known minimums, which are provided in their description. We will use these minimums later on **to compare** the coordinates we get from our optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINIMUM OF FUNCTION 1: 21.1\n",
      "MINIMUM OF FUNCTION 2: 26.91\n",
      "MINIMUM OF FUNCTION 3: 311.6\n",
      "MINIMUM OF FUNCTION 4: 311.6\n",
      "MINIMUM OF FUNCTION 5: -48.47\n",
      "MINIMUM OF FUNCTION 6: -91.36\n",
      "MINIMUM OF FUNCTION 7: 32.49\n",
      "MINIMUM OF FUNCTION 8: 71.6\n",
      "MINIMUM OF FUNCTION 9: -356.7\n",
      "MINIMUM OF FUNCTION 10: 51.03\n",
      "MINIMUM OF FUNCTION 11: -96.65\n",
      "MINIMUM OF FUNCTION 12: 553.39\n",
      "MINIMUM OF FUNCTION 13: 9.88\n",
      "MINIMUM OF FUNCTION 14: 405.47\n",
      "MINIMUM OF FUNCTION 15: 64.25\n",
      "MINIMUM OF FUNCTION 16: -43.28\n",
      "MINIMUM OF FUNCTION 17: 227.51\n",
      "MINIMUM OF FUNCTION 18: 227.51\n",
      "MINIMUM OF FUNCTION 19: 73.06\n",
      "MINIMUM OF FUNCTION 20: -123.81\n",
      "MINIMUM OF FUNCTION 21: -44.42\n",
      "MINIMUM OF FUNCTION 22: 222.1\n",
      "MINIMUM OF FUNCTION 23: -1000.0\n",
      "MINIMUM OF FUNCTION 24: -1.33\n"
     ]
    }
   ],
   "source": [
    "# initializing the functions and checking their minimums\n",
    "functions = {}\n",
    "true_minimums = {}\n",
    "\n",
    "for fun in range(1, 25):\n",
    "    functions[fun] = (smoof.makeBBOBFunction(40, fun, 2023))\n",
    "    true_minimums[fun] = smoof.getGlobalOptimum(functions[fun]).rx2(\"value\")[0]\n",
    "    print(f\"MINIMUM OF FUNCTION {fun}: {true_minimums[fun]}\")\n",
    "\n",
    "# setting the bounds:\n",
    "upper_bound = 5\n",
    "lower_bound = -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing minimums we got with the real ones\n",
    "def comparison(approx_minimums):\n",
    "    error = 0\n",
    "    for i,(true,approx) in enumerate(zip(true_minimums.values(), approx_minimums),1):\n",
    "        print(f\"{i : <20} TRUE: {true : <20} APPROXIMATE: {round(approx,2) : <20} DIFFERENCE: {round(abs(true - approx),2)}\")\n",
    "        error = error + round(abs(true - approx),2)\n",
    "    print(f\"OVERALL ABSOLUTE ERROR:  {error}\")\n",
    "\n",
    "# writing the coordinates of the minimum to file\n",
    "def results_to_file(coordinates, algo_number):\n",
    "    with open(f\"algorithm_{algo_number}.txt\", 'w') as f:\n",
    "        for point in coordinates:\n",
    "            for xi in point:\n",
    "                f.write(f\"{xi} \")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ALGORITHM 1:** Gradient Descent\n",
    "\n",
    "This algorithm was chosen as the **first one** we tested, under the assumtion it is often wildly used and works for functions that behave well. One big problem is the differentiability of the functions which is not always favourable and is sometimes time consuming so we used an **approximation**. 15 out of the 17 functions with description are described as **differentiable** so it is worth analysing this approach, even though it is a bit naive because it only performs well for smooth, convex, unimodal functions. The problem with this aproach is also that it has a big possibility of getting stuck in a **local minimum**. We will be solving this problem by adding a random component to it, making the algorithm go through **more iterations** starting at **different points** chosen uniformly at random.\n",
    "\n",
    "The idea is that we start in a randomly chosen point and compute the **gradient** for that set of coordinates. The function **``gradient_approximation``** takes care of that. Instead of differentiating each variable, we use two points that are 0.01 apart. Because this is the direction of the **biggest increase** for the function we are trying to optimize we compute the next point by going in the opposite direction of the mentioned increase. This is the equation $x_n = x_{n-1} - \\alpha \\cdot \\bigtriangleup G(x_n)$ where $\\bigtriangleup G$ is the gradient of the objective function.\n",
    "\n",
    "The next question that arises in this proposition is what is the **step size** in this direction? Let's look at the **``alpha``** parameter in the code bellow. Now there are many ways to find this scalar, the optimal one would be finding the one that **minimizes** the equation: $G(x_{n-1} - \\alpha \\cdot \\bigtriangleup G(x_n))$. This would take a lot of time because we are working with 40 dimentions and after some observations one can notice the elements of the gradients sometimes tend to be very different in size. We tackled with this problem by making the step size for each partial derivative different making it **between 0 and 1** for every single one. This also solves a lot of domain breach problems. We wrapped all of this up in a function that creates the new point **``new_point``**.\n",
    "\n",
    "***NOTE***: This algorithm is quite **time consuming** for all 24 functions, because we have to calculate the **gradient approximation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function implementation\n",
    "def gradient_descent(limit, dim, functions):\n",
    "    \n",
    "    # setting the best solution to the first one we try and updating it later on \n",
    "    # this is not neccessary and could be omited but given our lambda calculation we could skip to a worse solution\n",
    "    best_coordinates = np.random.uniform(lower_bound, upper_bound, (len(functions), dim))\n",
    "    best_values = [functions[i + 1](best_coordinates[i])[0] for i in range(len(functions))]\n",
    "\n",
    "    # our solution in each step - starting position\n",
    "    curr_coordinates = best_coordinates.copy()\n",
    "\n",
    "    # limit specifies the number of descents\n",
    "    for i in range(len(functions)):\n",
    "        for k in range(limit):\n",
    "            gradient = gradient_approximation(functions[i + 1], curr_coordinates[i], dim)\n",
    "            if gradient == -1: break\n",
    "\n",
    "            curr_coordinates[i] = new_point(curr_coordinates[i], gradient)\n",
    "            curr_value = functions[i + 1](curr_coordinates[i])[0]\n",
    "\n",
    "            if curr_value < best_values[i]:\n",
    "                best_values[i] = curr_value\n",
    "                best_coordinates[i] = curr_coordinates[i].copy()\n",
    "            \n",
    "    \n",
    "    return best_coordinates, best_values\n",
    "           \n",
    "# approximates all partial derivatives in the gradient            \n",
    "def gradient_approximation(function, x0, dim):\n",
    "    approx = []\n",
    "    x1 = x0.copy()\n",
    "\n",
    "    # approximation for each variable\n",
    "    for i in range(dim):\n",
    "        x1[i] = x0[i] + 0.01\n",
    "        y0 = function(x0)\n",
    "        y1 = function(x1)\n",
    "\n",
    "        # if the difference is too small we can increase the difference to move the point still\n",
    "        if math.isnan(y1 - y0): x1[i] = x1[i] + 0.04\n",
    "        approx.append(((y1 - y0)/(x1[i] - x0[i]))[0])\n",
    "        x1[i] = x0[i]\n",
    "    return approx\n",
    "\n",
    "# calculating the lambda and deciding on a new point\n",
    "def new_point(point, gradient):\n",
    "    new = []\n",
    "    for xi, grad in zip(point, gradient):\n",
    "\n",
    "        # calculating te number of digits in each derivative\n",
    "        no_digits = len(str(abs(math.floor(grad))))\n",
    "        alpha = 10**(-no_digits)\n",
    "        next_point = xi - alpha * grad\n",
    "\n",
    "        # checking the constraints\n",
    "        if next_point < upper_bound and next_point > lower_bound: new.append(next_point)\n",
    "        else: new.append(xi)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                    TRUE: 21.1                 APPROXIMATE: 21.1                 DIFFERENCE: 0.0\n",
      "2                    TRUE: 26.91                APPROXIMATE: 44999.24             DIFFERENCE: 44972.33\n",
      "3                    TRUE: 311.6                APPROXIMATE: 1695.34              DIFFERENCE: 1383.74\n",
      "4                    TRUE: 311.6                APPROXIMATE: 2232.08              DIFFERENCE: 1920.48\n",
      "5                    TRUE: -48.47               APPROXIMATE: -20.81               DIFFERENCE: 27.66\n",
      "6                    TRUE: -91.36               APPROXIMATE: -16.37               DIFFERENCE: 74.99\n",
      "7                    TRUE: 32.49                APPROXIMATE: 2755.95              DIFFERENCE: 2723.46\n",
      "8                    TRUE: 71.6                 APPROXIMATE: 639.85               DIFFERENCE: 568.25\n",
      "9                    TRUE: -356.7               APPROXIMATE: 3.2                  DIFFERENCE: 359.9\n",
      "10                   TRUE: 51.03                APPROXIMATE: 274948.96            DIFFERENCE: 274897.93\n",
      "11                   TRUE: -96.65               APPROXIMATE: 370.19               DIFFERENCE: 466.84\n",
      "12                   TRUE: 553.39               APPROXIMATE: 1273152.43           DIFFERENCE: 1272599.04\n",
      "13                   TRUE: 9.88                 APPROXIMATE: 287.88               DIFFERENCE: 278.0\n",
      "14                   TRUE: 405.47               APPROXIMATE: 405.71               DIFFERENCE: 0.24\n",
      "15                   TRUE: 64.25                APPROXIMATE: 646.46               DIFFERENCE: 582.21\n",
      "16                   TRUE: -43.28               APPROXIMATE: -7.51                DIFFERENCE: 35.77\n",
      "17                   TRUE: 227.51               APPROXIMATE: 233.71               DIFFERENCE: 6.2\n",
      "18                   TRUE: 227.51               APPROXIMATE: 264.36               DIFFERENCE: 36.85\n",
      "19                   TRUE: 73.06                APPROXIMATE: 101.6                DIFFERENCE: 28.54\n",
      "20                   TRUE: -123.81              APPROXIMATE: -122.59              DIFFERENCE: 1.22\n",
      "21                   TRUE: -44.42               APPROXIMATE: -42.34               DIFFERENCE: 2.08\n",
      "22                   TRUE: 222.1                APPROXIMATE: 307.41               DIFFERENCE: 85.31\n",
      "23                   TRUE: -1000.0              APPROXIMATE: -994.61              DIFFERENCE: 5.39\n",
      "24                   TRUE: -1.33                APPROXIMATE: 943.13               DIFFERENCE: 944.46\n",
      "OVERALL ABSOLUTE ERROR:  1602000.8900000001\n"
     ]
    }
   ],
   "source": [
    "# testing our algorithm\n",
    "# we make multiple runs \n",
    "# IMPORTANT: DONT OVERWRITE FINAL RESULTS !!!\n",
    "min_values = [float('inf') for i in range(len(functions))]\n",
    "min_coordinates = [[0 for i in range(40)] for k in range(len(functions))]\n",
    "no_runs = 10 \n",
    "\n",
    "for i in range(no_runs):\n",
    "    curr_coordinates, curr_values = gradient_descent(limit=100, dim=40, functions=functions)\n",
    "    for k in range(len(functions)):\n",
    "        if curr_values[k] < min_values[k]:\n",
    "            min_values[k] = curr_values[k]\n",
    "            min_coordinates[k] = curr_coordinates[k].copy()\n",
    "    \n",
    "comparison(min_values)\n",
    "results_to_file(min_coordinates, 1)\n",
    "\n",
    "# PARAMETERS \n",
    "# 1 run with limit 100: 1 min - testing\n",
    "# 10 runs with limit 100: 10 min - decent result\n",
    "# 100 runs with limit 300: 200 min - final result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking these results in to consideration we could deffenitly do better. We can see that in general this algorithm **does not perform well time-wise**, at least not given these parameters.\n",
    "While it does produce quite good results for some fuctions, we can see it fails most drastically in the case of **function 2** and **12**. Let us try a **local search** approach next to see if we could perhaps imporve these results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ALGORITHM 2:** Simulated Annealing\n",
    "\n",
    "This was the **second one** algorithm. The reason behind this one was the fact that these functions are very different, so the idea of some improved **random search** could work if we are trying to get the best possible result for all functions even though this could mean cutting some loses at those more specific ones. \n",
    "\n",
    "Now for a quick summery of how this algorithm works. We start with random points for each function. At every iteration of the algorithm take a **random step** between -0.1 and 0.1 for all 40 dimensions and check whether this **neighbour** produces a **better objective value**. Now in order not to get stuck in a **local minimum** we decide to move to this new point even if the solution is **worse** with some **probability** that is getting smaller with each iteration. This probability is regulated with a parameter called **``temperature``**. The higher the temperature the bigger the probability of accepting the bad solution, the more search space we explore. The lower the temperature, the more we focus on the solution at hand and optimizing this one. \n",
    "\n",
    "In our code **``temp``** is the initial temperature which is used to calculate the actual temperature in each step of the loop. The condition that decides the probability is taken from the Metropolis algorithm and goes as follows: \n",
    "1. first we calculate the difference between the current value and the candidate value of the neighbour: $\\bigtriangleup$\n",
    "2. if the difference is a negative one then the solution is better and we accept and save it - this is now our current position\n",
    "3. otherwise we accept the probability with the Boltzmann distribution: $P = e^{\\cfrac {\\bigtriangleup} {temperature}}$\n",
    "4. we generate a random number and if the number is higher we move to the candidate point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation\n",
    "def simulated_annealing(limit, dim, temp, functions):\n",
    "    \n",
    "    # setting the best solution to the first one we try and updating it later on\n",
    "    best_coordinates = np.random.uniform(lower_bound, upper_bound, (len(functions), dim))\n",
    "    best_values = [functions[i + 1](best_coordinates[i])[0] for i in range(len(functions))]\n",
    "\n",
    "    # our solution in each step - starting position\n",
    "    curr_coordinates = best_coordinates.copy()\n",
    "    curr_values = best_values.copy()\n",
    "    \n",
    "    for i in range(len(functions)):\n",
    "        for k in range(limit):\n",
    "\n",
    "            # calculating the neighbour candidate and checking the bounds\n",
    "            candidate = curr_coordinates[i] + np.random.uniform(-0.1, 0.1, 40)\n",
    "            if all(xi < upper_bound and xi > lower_bound for xi in candidate):\n",
    "                \n",
    "                # if the value is better than the best one so far we save it\n",
    "                candidate_value = functions[i + 1](candidate)[0]\n",
    "                if candidate_value < best_values[i]:\n",
    "                    best_coordinates[i] = candidate.copy()\n",
    "                    best_values[i] = candidate_value.copy()\n",
    "                \n",
    "                # calculating the difference between the values and the new temperature\n",
    "                diff = candidate_value - curr_values[i]\n",
    "                temperature = temp / float(k + 1)\n",
    "\n",
    "                # calculating the probability of accepting a bad solution\n",
    "                x = -diff / temperature\n",
    "                try: condition = math.exp(x)\n",
    "                except OverflowError: condition = float(\"Inf\")\n",
    "\n",
    "                # random throw whether we accept the solution\n",
    "                if diff < 0 or np.random.random() < condition:\n",
    "                    curr_coordinates[i] = candidate.copy()\n",
    "                    curr_values[i] = candidate_value.copy()\n",
    "            \n",
    "    return [best_coordinates, best_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                    TRUE: 21.1                 APPROXIMATE: 21.47                DIFFERENCE: 0.37\n",
      "2                    TRUE: 26.91                APPROXIMATE: 67747.44             DIFFERENCE: 67720.53\n",
      "3                    TRUE: 311.6                APPROXIMATE: 1244.59              DIFFERENCE: 932.99\n",
      "4                    TRUE: 311.6                APPROXIMATE: 1611.71              DIFFERENCE: 1300.11\n",
      "5                    TRUE: -48.47               APPROXIMATE: 211.75               DIFFERENCE: 260.22\n",
      "6                    TRUE: -91.36               APPROXIMATE: -14.13               DIFFERENCE: 77.23\n",
      "7                    TRUE: 32.49                APPROXIMATE: 765.99               DIFFERENCE: 733.5\n",
      "8                    TRUE: 71.6                 APPROXIMATE: 122.15               DIFFERENCE: 50.55\n",
      "9                    TRUE: -356.7               APPROXIMATE: -302.92              DIFFERENCE: 53.78\n",
      "10                   TRUE: 51.03                APPROXIMATE: 44245.2              DIFFERENCE: 44194.17\n",
      "11                   TRUE: -96.65               APPROXIMATE: 144.2                DIFFERENCE: 240.85\n",
      "12                   TRUE: 553.39               APPROXIMATE: 115306.46            DIFFERENCE: 114753.07\n",
      "13                   TRUE: 9.88                 APPROXIMATE: 86.84                DIFFERENCE: 76.96\n",
      "14                   TRUE: 405.47               APPROXIMATE: 405.86               DIFFERENCE: 0.39\n",
      "15                   TRUE: 64.25                APPROXIMATE: 1814.84              DIFFERENCE: 1750.59\n",
      "16                   TRUE: -43.28               APPROXIMATE: -27.99               DIFFERENCE: 15.29\n",
      "17                   TRUE: 227.51               APPROXIMATE: 242.32               DIFFERENCE: 14.81\n",
      "18                   TRUE: 227.51               APPROXIMATE: 291.23               DIFFERENCE: 63.72\n",
      "19                   TRUE: 73.06                APPROXIMATE: 90.35                DIFFERENCE: 17.29\n",
      "20                   TRUE: -123.81              APPROXIMATE: -121.37              DIFFERENCE: 2.44\n",
      "21                   TRUE: -44.42               APPROXIMATE: -42.34               DIFFERENCE: 2.08\n",
      "22                   TRUE: 222.1                APPROXIMATE: 225.08               DIFFERENCE: 2.98\n",
      "23                   TRUE: -1000.0              APPROXIMATE: -995.79              DIFFERENCE: 4.21\n",
      "24                   TRUE: -1.33                APPROXIMATE: 1129.35              DIFFERENCE: 1130.68\n",
      "OVERALL ABSOLUTE ERROR:  233398.81\n"
     ]
    }
   ],
   "source": [
    "# testing our algorithm\n",
    "# we make multiple runs \n",
    "# IMPORTANT: DONT OVERWRITE FINAL RESULTS !!!\n",
    "min_values_2 = [float('inf') for i in range(len(functions))]\n",
    "min_coordinates_2 = [[0 for i in range(40)] for k in range(len(functions))]\n",
    "no_runs = 10\n",
    "\n",
    "for i in range(no_runs):\n",
    "    curr_coordinates, curr_values = simulated_annealing(limit=3000, dim=40, temp=100, functions=functions)\n",
    "    for k in range(len(functions)):\n",
    "        if curr_values[k] < min_values_2[k]:\n",
    "            min_values_2[k] = curr_values[k] \n",
    "            min_coordinates_2[k] = curr_coordinates[k].copy()\n",
    "            \n",
    "\n",
    "comparison(min_values_2)\n",
    "results_to_file(min_coordinates_2, 2)\n",
    "# PARAMETERS \n",
    "# 10 runs with limit 3000: 3 min - testing\n",
    "# 100 runs with limit 3000: 30 min - final result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is **overall better**, if we take the absolute error as a measurment of succsess, but gradient descent performed better for some **specific cases**. The difference in the result is mainly due to different results in the case of **function 12**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ALGORITHM 3:** Nika Go Wild â™¡\n",
    "- v tretji celici mas izpis za primerjavo in izpis kordinat v datoteko tko kokr je on hotu\n",
    "- in general mas ``functions[]`` array vseh funkcij das is it \n",
    "- tko implementacija vsega kar rabs je mainly na zacetku\n",
    "- zaenkrat komot laufas une funkcije ko pise ``DO NOT OVERWRITE`` ampak u soboto okoli 1 bom pushlna gor final rezultate za te algoritme tko da mejbi ne jih laufat kr jih bom pustila cez noc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
